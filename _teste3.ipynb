{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pettingzoo.classic import texas_holdem_v4\n",
    "\n",
    "env = texas_holdem_v4.env(render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9, s_10, s_11, s_12, s_13, s_14, s_15, s_16, s_17, s_18, s_19, s_20, s_21, s_22, s_23, s_24, s_25, s_26, s_27, s_28, s_29, s_30, s_31, s_32, s_33, s_34, s_35, s_36, s_37, s_38, s_39, s_40, s_41, s_42, s_43, s_44, s_45, s_46, s_47, s_48, s_49, s_50, s_51, s_52, s_53, s_54, s_55, s_56, s_57, s_58, s_59, s_60, s_61, s_62, s_63, s_64, s_65, s_66, s_67, s_68, s_69, s_70, s_71, a, v, c]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "class Q:\n",
    "    def __init__(self):\n",
    "        self.q = pd.DataFrame(columns=[f's_{i}' for i in range(72)] + ['a', 'v', 'c'])\n",
    "\n",
    "    def find(self, s, a=None):\n",
    "        filtered_rows = True\n",
    "        for i,v in enumerate(s):\n",
    "            current_filter = (self.q[f's_{i}'] == v)\n",
    "            filtered_rows &= current_filter\n",
    "        if a is None:\n",
    "            return filtered_rows\n",
    "        else:\n",
    "            return (filtered_rows & (self.q['a'] == a))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s,a = index\n",
    "        v = self.q.loc[self.find(s,a)].v\n",
    "        if len(v.values):\n",
    "            return v.item()\n",
    "        else:\n",
    "            return 0\n",
    "    def __setitem__(self, index, value):\n",
    "        s,a = index\n",
    "        if value == 0:\n",
    "            return\n",
    "        \n",
    "        cases = self.q.loc[self.find(s,a)]\n",
    "        if cases.shape[0]>0:\n",
    "            # cases['v'] = value\n",
    "            # cases['c'] = cases.c.item()+1\n",
    "            self.q.loc[self.find(s,a), ['v','c']] = [value, cases.c.item()+1]\n",
    "        else:\n",
    "            if self.q.shape[0] < 1e9:\n",
    "                self.q.loc[self.q.shape[0]] = [i for i in s]+[a, value, 1]\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.q.to_string()\n",
    "\n",
    "\n",
    "q = Q()\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.wins = 0\n",
    "        self.acum = 0\n",
    "\n",
    "        self.hist = []\n",
    "        self.current_reward = []\n",
    "\n",
    "        self.n = 0\n",
    "        self.avg = 0\n",
    "    \n",
    "    def reward(self, r):\n",
    "        if r != 0:\n",
    "            self.hist.append(r)\n",
    "        self.current_reward.append(r)\n",
    "    \n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.acum += sum(self.current_reward)\n",
    "        self.wins += int(sum(self.current_reward)>0)\n",
    "        self.n += 1\n",
    "        self.avg += (sum(self.current_reward) - self.avg) / self.n\n",
    "        self.current_reward=[]\n",
    "\n",
    "    def act(self, s, mask, best=False):\n",
    "        if mask[0]: # Call\n",
    "            return 0 \n",
    "        if mask[3]: # Check\n",
    "            return 3 \n",
    "        if mask[2]: # Fold\n",
    "            return 2 \n",
    "\n",
    "class Player(BasePlayer):\n",
    "    def __init__(self, name, q=None, gamma=.9, alpha=.9):\n",
    "        self.Q = (q or Q())\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = {}\n",
    "        super().__init__(name)\n",
    "\n",
    "    def learn(self, s, a, r, s_):        \n",
    "        if s is not None and a is not None:\n",
    "            if tuple(s) not in self.buffer:\n",
    "                self.buffer[tuple(s)] = [self.Q[s,x] for x in range(4)]\n",
    "            if tuple(s_) not in self.buffer:\n",
    "                self.buffer[tuple(s_)] = [self.Q[s_,x] for x in range(4)]\n",
    "            \n",
    "            delta = r + self.gamma*np.max(self.buffer[tuple(s_)]) - self.buffer[tuple(s)][a]\n",
    "            self.Q[s,a] =  self.buffer[tuple(s)][a] + self.alpha*delta\n",
    "\n",
    "        super().learn(s, a, r, s_)\n",
    "\n",
    "    def act(self, s, mask):\n",
    "        # s = binary_to_decimal(s)\n",
    "        if tuple(s) not in self.buffer:\n",
    "            self.buffer = { tuple(s): [self.Q[s,a] for a in range(4)]}\n",
    "            \n",
    "        v = np.array([a if mask[i] else -np.inf for i, a in enumerate(self.buffer[tuple(s)])])\n",
    "        return np.random.choice(np.flatnonzero(v == v.max()))\n",
    "        # return np.argmax([self.Q[s,a] if mask[a] else -np.inf for a in range(4)])\n",
    "        \n",
    "        \n",
    "agents = {name: Player(name) for name in ['player_0', 'player_1'] }\n",
    "# agents = {\n",
    "#     'player_0': Player('player_0', .9, .9),\n",
    "#     'player_1': BasePlayer('player_1')  \n",
    "# }\n",
    "\n",
    "# {name: agent.acum for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agents, max_iter):\n",
    "    s, a = None, None\n",
    "    for rounds in range(max_iter):\n",
    "        env.reset(seed=42)\n",
    "        # print(rounds)\n",
    "        for agent in env.agent_iter():\n",
    "            player = agents[agent]\n",
    "            observation, r, termination, truncation, info = env.last()\n",
    "            \n",
    "            s_ = observation['observation']\n",
    "            mask = observation[\"action_mask\"]\n",
    "            \n",
    "            player.reward(r)\n",
    "            if termination or truncation:\n",
    "                a_ = None\n",
    "            else:\n",
    "                a_ = player.act(s_, mask)\n",
    "\n",
    "            player.learn(s,a,r,s_)\n",
    "            s = s_\n",
    "            a = a_\n",
    "\n",
    "            # print(s, a, r)\n",
    "            env.step(a)\n",
    "    env.close()\n",
    "\n",
    "# {name: agent.wins for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = int(1e1)\n",
    "gamma = .999\n",
    "alpha = .99\n",
    "\n",
    "shared_q = Q()\n",
    "agents = {\n",
    "    'player_0': Player('player_0', shared_q, gamma, alpha),\n",
    "    # 'player_1': BasePlayer('player_1')  \n",
    "    # 'player_1': Player('player_0', gamma, alpha),\n",
    "    'player_1': Player('player_0', shared_q, gamma, alpha),\n",
    "}\n",
    "\n",
    "# train(max_iter)\n",
    "# {name: agent.wins for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 999, 'player_1': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iter = int(1e3)\n",
    "train(agents, max_iter)\n",
    "{name: agent.wins for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 999.0, 'player_1': -999.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{name: agent.acum for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# hist = pd.DataFrame(\n",
    "#     {\"r\": agents['player_0'].hist}\n",
    "# )\n",
    "# hist.to_csv('qlearning_p0.csv', index=False)\n",
    "\n",
    "# hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainado vs Basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 0, 'player_1': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_agents = {\n",
    "    'player_0': Player('player_0', shared_q, gamma, alpha),\n",
    "    'player_1': BasePlayer('player_1')  \n",
    "}\n",
    "train(new_agents, 100)\n",
    "{name: agent.wins for name, agent in new_agents.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
