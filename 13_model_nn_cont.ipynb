{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.river import River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Esimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(nn.Module):\n",
    "    def __init__(self, inputs, outputs, hidden_size=10):\n",
    "        super(Estimator, self).__init__()\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(len(inputs), hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, len(outputs))\n",
    "\n",
    "    def encode_inputs(self, *data):\n",
    "        # return torch.cat([\n",
    "        #     nn.functional.one_hot(torch.tensor([x]), num_classes=size)\n",
    "        #     for x, size in zip(data, self.inputs)\n",
    "        # ], dim=1).float()\n",
    "        return torch.tensor(data).float()\n",
    "    def decode_output(self, data):\n",
    "        return torch.clamp(data, min=0.0, max=sum(self.outputs)-1)\n",
    "\n",
    "    def train(self, loss, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        x = self.encode_inputs(*inputs)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.relu(x)\n",
    "        # return self.decode_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "#### Estimadores: \n",
    " \n",
    "$ \\{ \\hat{T}, \\hat{R} \\} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, S,A, learning_rate=0.01):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "\n",
    "        self.estimator = Estimator([len(S),len(A)], [len(S)])\n",
    "\n",
    "        self.optimizer = optim.SGD(self.estimator.parameters(), lr=learning_rate)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.estimatives = None\n",
    "        \n",
    "        self.r = np.zeros((len(S),len(A),len(S)))\n",
    "    \n",
    "    def loss(self, *data):\n",
    "        obs = torch.tensor(data).float().unsqueeze(0)\n",
    "        obs = obs[:,2]\n",
    "        # pred = (pred or torch.tensor([pred]).float()) \n",
    "        loss = self.loss_function(self.estimatives, obs)\n",
    "        return loss\n",
    "\n",
    "    def learn(self, s,a,s_,r):\n",
    "        # Learn T model\n",
    "        loss = self.loss(s,a,s_,r)\n",
    "        self.estimator.train(loss, self.optimizer)\n",
    "        # self.estimatives = None\n",
    "        \n",
    "        # Learn R model\n",
    "        self.r[s,a,s_] = r\n",
    "        return loss.item()\n",
    "    \n",
    "    def predict(self, s, a, register=False):\n",
    "        if register:\n",
    "            s_ = self.estimator.forward(s,a)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s_ = self.estimator.forward(s,a)\n",
    "        \n",
    "        self.estimatives = s_\n",
    "        s_ = torch.round(torch.clamp(self.estimator.relu(s_), min=0.0, max=len(self.S)-1)).int().item()\n",
    "        return s_, self.r[s,a, s_]\n",
    "\n",
    "# model = Model(range(10), range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorítimo de Controle (RL):\n",
    " - Value Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, model, gamma=.9, alpha=.9, max_iter=10):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.Q = np.zeros((self.model.S.size, self.model.A.size))\n",
    "        self.control()\n",
    "\n",
    "    def max_random(self, x):\n",
    "        return np.random.choice(np.flatnonzero(x == x.max()))\n",
    "\n",
    "    def predict(self):\n",
    "        s = np.random.choice(self.model.S)\n",
    "        for i in range(self.max_iter):\n",
    "            a = self.max_random(self.Q[s])\n",
    "            s_, r = self.model.predict(s, a)\n",
    "\n",
    "            delta = r + self.gamma*np.max([self.Q[s_,a_] for a_ in self.model.A]) - self.Q[s,a]\n",
    "            self.Q[s,a] +=  self.alpha*delta\n",
    "\n",
    "            s = s_\n",
    "\n",
    "    def control(self):\n",
    "        self.pi = [self.max_random(s) for s in self.Q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, S, A, epsilon=0.3, gamma=.9, alpha=.9, max_iter=10, learning_rate=0.01):\n",
    "        self.model = Model(S, A, learning_rate=learning_rate)\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.rl = QLearning(self.model, gamma, alpha, max_iter)\n",
    "        self.err = np.ones((S.size, A.size))\n",
    "        self.guess = None\n",
    "    \n",
    "    def learn(self, s,a,s_,r):\n",
    "        need_improve = (self.err[s,a] > self.epsilon)\n",
    "        if need_improve:\n",
    "            for i in range(self.max_iter):\n",
    "                pred = self.model.predict(s, a, register=need_improve)\n",
    "                err = self.model.learn(s,a,s_,r)\n",
    "                self.err[s,a] = err\n",
    "                if self.err[s,a] < self.epsilon:\n",
    "                    break\n",
    "        else:\n",
    "            self.err[s,a] = (self.guess-s_)**2\n",
    "        \n",
    "        self.rl.predict()\n",
    "        self.rl.control()\n",
    "\n",
    "    def act(self, s):\n",
    "        pi = self.policy()\n",
    "        a = pi[s]\n",
    "        self.guess = self.model.predict(s, a)[0]\n",
    "        return a\n",
    "        \n",
    "    def v(self):\n",
    "        return [np.max(s) for s in self.rl.Q]\n",
    "    def q(self):\n",
    "        return self.rl.Q\n",
    "    def policy(self):\n",
    "        vals = sp.special.softmax(self.rl.Q, axis=1)\n",
    "        weights = sp.special.softmax(self.err * vals, axis=1)\n",
    "        pi = [np.random.choice(range(weights.shape[1]), p=w) for w in weights] \n",
    "        return pi\n",
    "        # return self.rl.pi\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env,agent, size_limit=100):\n",
    "    data = []\n",
    "    env.reset()\n",
    "    for t in range(size_limit):\n",
    "        # print('t: ',t)\n",
    "        # print('act')\n",
    "        s, _, _, _, _ = env.last()\n",
    "        a = agent.act(s)\n",
    "        s_, r, end, _, _ = env.step(s,a)\n",
    "        \n",
    "        # print('learn')\n",
    "        step = (s,a,s_,r) \n",
    "        agent.learn(*step) \n",
    "        data.append(step)\n",
    "        \n",
    "        # print('restart')\n",
    "        if end: break\n",
    "    return data\n",
    "\n",
    "def experiment(env, agent, max_iterations=1000, episode_sizes=100):\n",
    "    data = []\n",
    "    for i in range(max_iterations):\n",
    "        epi = generate_episode(env, agent, episode_sizes)\n",
    "        # print(f'{i}: {len(epi)}')\n",
    "        # if len(epi) < 12:\n",
    "        #     print(f'{i}: {len(epi)}')\n",
    "        data.append(epi)    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "env = River()\n",
    "\n",
    "learning_rate=0.9\n",
    "gamma = 0.9\n",
    "alpha = 0.9\n",
    "epsilon = 0.2\n",
    "max_iter = 10\n",
    "num_episodes = 100\n",
    "episode_size = 12\n",
    "\n",
    "agent = Agent(env.S, env.A, epsilon, gamma, alpha, max_iter)\n",
    "print([agent.model.predict(0,a)[0] for a in env.A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 0]\n",
      "[2, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print([agent.model.predict(0,a)[0] for a in env.A])\n",
    "exp = experiment(env, agent, num_episodes, episode_size)\n",
    "print([agent.model.predict(0,a)[0] for a in env.A])\n",
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0]\n",
      " [3 1 0 0]\n",
      " [3 3 0 0]\n",
      " [3 3 2 0]\n",
      " [3 3 3 0]\n",
      " [3 3 3 2]\n",
      " [3 3 3 3]\n",
      " [3 3 3 3]\n",
      " [3 3 3 3]\n",
      " [3 3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([[agent.model.predict(s,a)[0] for a in env.A] for s in env.S]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.round(agent.q(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____________________________ \n",
      "| -0.0| -0.0| -0.0| -0.0| -0.0|\n",
      "|_____|_____|_____|_____|_____|\n",
      "| -0.1| -0.0| -0.0| -0.0| -0.0|\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n",
      " _____________________________ \n",
      "|  ↓  |  ↓  |  →  |  ↑  |  ↓  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↓  |  ↑  |  ↑  |  ↑  |  ↓  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# env.plot([max(s) for s in np.round(agent.q(), 1)])\n",
    "env.plot(list(np.round(agent.v(), 1)))\n",
    "env.plot(agent.policy(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
