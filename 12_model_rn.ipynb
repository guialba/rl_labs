{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.river import River"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Esimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        super(Estimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        # self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def train(self, loss, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class EstimatorT(Estimator):\n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        super(EstimatorT, self).__init__(input_size, output_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(EstimatorT, self).forward(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "#### Estimadores: \n",
    " \n",
    "$ \\{ \\hat{T}, \\hat{R} \\} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, S,A, learning_rate=0.01):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "\n",
    "        self.t_estimator = EstimatorT(len(S)+len(A), len(S))\n",
    "        \n",
    "        self.build_t()\n",
    "\n",
    "        self.optimizer_t = optim.SGD(self.t_estimator.parameters(), lr=learning_rate)\n",
    "        self.loss_t = nn.NLLLoss()\n",
    "        self.estimatives_t = None\n",
    "        \n",
    "        self.r = np.zeros((len(S),len(A),len(S)))\n",
    "\n",
    "    def build_input(self, s, a):\n",
    "        return torch.cat([\n",
    "            nn.functional.one_hot(torch.tensor([s]), num_classes=len(self.S)),\n",
    "            nn.functional.one_hot(torch.tensor([a]), num_classes=len(self.A))\n",
    "        ], dim=1).float()\n",
    "    \n",
    "    def build_t(self):\n",
    "        with torch.no_grad():\n",
    "            p = lambda s,a : self.t_estimator.forward(self.build_input(s,a))\n",
    "            self.t = np.array([[p(s,a)[0].tolist() for a in self.A] for s in self.S]) \n",
    "\n",
    "\n",
    "    def learn(self, s,a,s_,r):\n",
    "        # Learn T model\n",
    "        obs = torch.tensor((s,a,s_,r)).float().unsqueeze(0)\n",
    "        obs_t = obs[:,2]\n",
    "        pred_t = self.estimatives_t\n",
    "        lt = self.loss_t(pred_t, obs_t.long())\n",
    "        self.t_estimator.train(lt, self.optimizer_t)\n",
    "        self.estimatives_t = None\n",
    "\n",
    "        self.build_t()\n",
    "        \n",
    "        # Learn R model\n",
    "        self.r[s,a,s_] = r\n",
    "    \n",
    "    def predict(self, s, a, register=False):\n",
    "        def estimate_s():\n",
    "            x = self.build_input(s,a)\n",
    "            p = self.t_estimator.forward(x)\n",
    "            distribution = torch.distributions.Categorical(p)\n",
    "            s_ = distribution.sample()\n",
    "            # log_prob = distribution.log_prob(s_)\n",
    "            return s_, torch.log(p)\n",
    "\n",
    "        if register:\n",
    "            s_, log_probs = estimate_s()\n",
    "            self.estimatives_t = log_probs\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s_, _ = estimate_s()\n",
    "\n",
    "        return s_.item()\n",
    "    \n",
    "    def T(self, s,a,s_):\n",
    "        return self.t[s,a,s_]\n",
    "    \n",
    "    def R(self, s,a,s_):\n",
    "        return self.r[s,a,s_]\n",
    "\n",
    "# model = Model(range(10), range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorítimo de Controle (RL):\n",
    " - Value Interation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value_Iteration:\n",
    "    def __init__(self, model, gamma=.9, epsilon=.1):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.v = np.zeros(self.model.S.size)\n",
    "        self.control()\n",
    "\n",
    "    def bellman(self, s,a):\n",
    "        return sum([self.model.T(s,a,s_)*(self.model.R(s,a,s_)+self.gamma*self.v[s_]) for s_ in self.model.S])\n",
    "    \n",
    "    def run(self):\n",
    "        while True:\n",
    "            v = self.v.copy()\n",
    "            for s in self.model.S:\n",
    "                self.v[s] = np.max([self.bellman(s,a) for a in self.model.A])\n",
    "            if np.linalg.norm(v-self.v, ord=np.inf) < (self.epsilon*(1-self.gamma))/(2*self.gamma):\n",
    "                break\n",
    "\n",
    "    def control(self):\n",
    "        self.pi = [np.argmax([np.sum([self.model.T(s,a,s_)*(self.model.R(s,a,s_) + self.gamma*self.v[s_]) for s_ in self.model.S]) for a in self.model.A]) for s in self.model.S]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, S, A, gamma=.9, epsilon=.1, learning_rate=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.model = Model(S, A, learning_rate=learning_rate)\n",
    "        self.rl = Value_Iteration(self.model, self.gamma, self.epsilon)\n",
    "    \n",
    "    def learn(self, s,a,s_,r):\n",
    "        self.model.learn(s,a,s_,r)\n",
    "        self.rl.run()\n",
    "        self.rl.control()\n",
    "\n",
    "    def act(self, s):\n",
    "        pi = self.get_policy()\n",
    "        self.model.predict(s, pi[s], register=True)\n",
    "        return pi[s]\n",
    "    def evaluate(self, s):\n",
    "        v = self.get_v()\n",
    "        return v[s]\n",
    "    \n",
    "    def get_v(self):\n",
    "        return self.rl.v\n",
    "    def get_policy(self):\n",
    "        return self.rl.pi\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env,agent, size_limit=100):\n",
    "    data = []\n",
    "    env.reset()\n",
    "    for t in range(size_limit):\n",
    "        # print('t: ',t)\n",
    "        s, _, _, _, _ = env.last()\n",
    "        a = agent.act(s)\n",
    "        s_, r, end, _, _ = env.step(s,a)\n",
    "        \n",
    "        step = (s,a,s_,r) \n",
    "        agent.learn(*step) \n",
    "        data.append(step)\n",
    "        if end: break\n",
    "    return data\n",
    "\n",
    "def experiment(env, agent, max_iterations=1000, episode_sizes=100):\n",
    "    data = []\n",
    "    for i in range(max_iterations):\n",
    "        epi = generate_episode(env, agent, episode_sizes)\n",
    "        # print(f'{i}: {len(epi)}')\n",
    "        data.append(len(epi))    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = River()\n",
    "\n",
    "learning_rate=0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "num_episodes = 100\n",
    "episode_size = 12\n",
    "\n",
    "agent = Agent(env.S, env.A, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0.09, 0.13, 0.07, 0.07, 0.07, 0.1, 0.15, 0.12, 0.08, 0.12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,a,s_=0,0,0\n",
    "# p = [agent.model.T(s,a,s_) for a in env.A]\n",
    "p = [np.round(agent.model.T(s,a,s_),2) for s_ in env.S]\n",
    "print(agent.model.predict(s,a))\n",
    "print(p)\n",
    "sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = experiment(env, agent, num_episodes, episode_size)\n",
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0.16, 0.22, 0.05, 0.03, 0.03, 0.27, 0.09, 0.06, 0.05, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,a,s_=0,0,0\n",
    "# p = [agent.model.T(s,a,s_) for a in env.A]\n",
    "p = [np.round(agent.model.T(s,a,s_),2) for s_ in env.S]\n",
    "print(agent.model.predict(s,a))\n",
    "print(p)\n",
    "sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____________________________ \n",
      "| -2.1| -2.1| -2.0| -2.0| -2.0|\n",
      "|_____|_____|_____|_____|_____|\n",
      "| -2.2| -2.4| -2.3| -2.3| -1.9|\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n",
      " _____________________________ \n",
      "|  ←  |  →  |  ↑  |  →  |  ↑  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↓  |  →  |  ↑  |  ↑  |  ↓  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.plot(list(np.round(agent.get_v(), 1)))\n",
    "env.plot(agent.get_policy(), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
