{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pettingzoo.classic import texas_holdem_v4\n",
    "\n",
    "env = texas_holdem_v4.env(render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        super(Pi, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "input_size = 1\n",
    "output_size = 4\n",
    "\n",
    "model = Pi(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 0, 'player_1': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasePlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.wins = 0\n",
    "        self.acum = 0\n",
    "\n",
    "        self.current_reward = []\n",
    "\n",
    "        self.n = 0\n",
    "        self.avg = 0\n",
    "    \n",
    "    def reward(self, r):\n",
    "        self.current_reward.append(r)\n",
    "    \n",
    "    def learn(self):\n",
    "        self.acum += sum(self.current_reward)\n",
    "        self.wins += int(sum(self.current_reward)>0)\n",
    "        self.n += 1\n",
    "        self.avg += (sum(self.current_reward) - self.avg) / self.n\n",
    "        self.current_reward=[]\n",
    "\n",
    "    def act(self, s, mask, best=False):\n",
    "        if mask[0]: # Call\n",
    "            return 0 \n",
    "        if mask[3]: # Check\n",
    "            return 3 \n",
    "        if mask[2]: # Fold\n",
    "            return 2 \n",
    "\n",
    "class Player(BasePlayer):\n",
    "    def __init__(self, name, pi=None, gamma=.9, learning_rate=1e-5):\n",
    "        # self.pi = Pi(72+4, 4) # mask as input\n",
    "        self.pi = (pi or Pi(72, 4))\n",
    "        self.optimizer = optim.SGD(self.pi.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.current_mask = None\n",
    "        self.current_data = []\n",
    "        super().__init__(name)\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.current_data) > 0 and sum(self.current_reward) > 0:\n",
    "            returns = torch.tensor([self.gamma**t * sum([self.gamma**(k-t) * r for k, r in enumerate(self.current_reward[t:])]) for t, _ in enumerate(self.current_reward)])\n",
    "            \n",
    "            policy_loss = [-log_prob * R for log_prob, R in zip(self.current_data, returns[:-1])]\n",
    "            policy_loss = torch.cat(policy_loss).sum()\n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.current_data=[]\n",
    "        super().learn()\n",
    "\n",
    "    def act(self, s, mask, best=False):\n",
    "        state = torch.from_numpy(s).float().unsqueeze(0)\n",
    "        self.current_mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        # probs = self.pi(torch.cat((state, m), axis=1)) # mask as inputs\n",
    "        masked = self.pi(state)*self.current_mask\n",
    "        probs = masked/torch.sum(masked)\n",
    "        \n",
    "        if best:\n",
    "            return torch.argmax(probs)\n",
    "        else:\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action = m.sample()\n",
    "            self.current_data.append(m.log_prob(action))\n",
    "            return action.item()\n",
    "        \n",
    "\n",
    "names = ['player_0', 'player_1'] \n",
    "agents = {name: Player(name) for name in names}\n",
    "{name: agent.acum for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(players):\n",
    "    s = env.reset(seed=42)\n",
    "    winner = None\n",
    "    for agent in env.agent_iter():\n",
    "        player = players[agent]\n",
    "        observation, r, termination, truncation, info = env.last()\n",
    "        s = observation['observation']\n",
    "        mask = observation[\"action_mask\"]\n",
    "        \n",
    "        player.reward(r)\n",
    "        if termination or truncation:\n",
    "            a = None\n",
    "        else:\n",
    "            a = player.act(s, mask)\n",
    "        env.step(a)\n",
    "\n",
    "        if r>0:\n",
    "            winner = player\n",
    "    return winner\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def reinforce(players, max_iter=1000):\n",
    "    for i in range(max_iter):\n",
    "        generate_episode(players)\n",
    "        \n",
    "        for player in players.values():\n",
    "            player.learn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = int(1e4)\n",
    "learning_rate = 1e-5\n",
    "gamma = 0.999\n",
    "\n",
    "# agents = {name: Player(name, gamma, learning_rate) for name in ['player_0', 'player_1'] }\n",
    "shared_pi = Pi(72, 4)\n",
    "agents = {\n",
    "    'player_0': Player('player_0', shared_pi, gamma, learning_rate),\n",
    "    # 'player_1': Player('player_1', gamma, learning_rate)  \n",
    "    'player_1': Player('player_1', shared_pi, gamma, learning_rate)  \n",
    "}\n",
    "# reinforce(agents, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 4048, 'player_1': 5952}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = int(1e5)\n",
    "reinforce(agents, num_episodes)\n",
    "{name: agent.wins for name, agent in agents.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainado vs Basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_0': 0, 'player_1': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = {\n",
    "    'player_0': Player('player_0', shared_pi, gamma, learning_rate),\n",
    "    'player_1': BasePlayer('player_1')  \n",
    "}\n",
    "\n",
    "reinforce(agents, 1)\n",
    "{name: agent.wins for name, agent in agents.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
